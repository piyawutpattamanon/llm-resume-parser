{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "resumes = []\n",
    "with open('Entity Recognition in Resumes.jsonl') as f:\n",
    "    for line in f:\n",
    "        resume = json.loads(line)\n",
    "        resumes.append(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_index = 10\n",
    "resume = resumes[resume_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "tokenizer = TokenTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "chunks = tokenizer.split_text(resume['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asish Ratha\n",
      "Subject matter Expert - Accenture\n",
      "\n",
      "Chennai, Tamil Nadu - Email me on Indeed: indeed.com/r/Asish-Ratha/853988e0e0e236a3\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Subject matter Expert\n",
      "\n",
      "Accenture -\n",
      "\n",
      "March 2012 to Present\n",
      "\n",
      "Subject matter expert\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "Berhampur university, Khallikote autonomous college -  Brahmapur, Orissa\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Invoice (5 years), posting. (5 years), TRAINING (4 years)\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "SKILLS\n",
      "Invoice processing, Team handling, new joiners training.sap posting,vendor call attend and\n",
      "resolve the issue,meet SLA tat,working with client tool.\n",
      "\n",
      "https://www.indeed.com/r/Asish-Ratha/853988e0e0e236a3?isid=rex-download&ikw=download-top&co=IN\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print('=' * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>220.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>943.231818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>741.759894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>134.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>440.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>751.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1176.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4682.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count   220.000000\n",
       "mean    943.231818\n",
       "std     741.759894\n",
       "min     134.000000\n",
       "25%     440.500000\n",
       "50%     751.000000\n",
       "75%    1176.750000\n",
       "max    4682.000000"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "tokenizer = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n",
    "token_sizes = [len(tokenizer.split_text(resume['content'])) for resume in resumes]\n",
    "\n",
    "df = pandas.DataFrame(token_sizes)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asish Ratha\n",
      "Subject matter Expert - Accenture\n",
      "\n",
      "Chennai, Tamil Nadu - Email me on Indeed: indeed.com/r/Asish-Ratha/853988e0e0e236a3\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Subject matter Expert\n",
      "\n",
      "Accenture -\n",
      "\n",
      "March 2012 to Present\n",
      "\n",
      "Subject matter expert\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "Berhampur university, Khallikote autonomous college -  Brahmapur, Orissa\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Invoice (5 years), posting. (5 years), TRAINING (4 years)\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "SKILLS\n",
      "Invoice processing, Team handling, new joiners training.sap posting,vendor call attend and\n",
      "resolve the issue,meet SLA tat,working with client tool.\n",
      "\n",
      "https://www.indeed.com/r/Asish-Ratha/853988e0e0e236a3?isid=rex-download&ikw=download-top&co=IN\n"
     ]
    }
   ],
   "source": [
    "print(resume['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_content = \"\"\"\n",
    " Piyawut Pattamanon (66) 86-708-1283   samnarak@gmail.com\n",
    "Summary\n",
    "I'm a geek who enjoys using technical solutions to solve real-world business problems and boost ROI. I specialize in optimizing engineering processes and managing cloud infrastructures (AWS, GCP) for big data environments and machine learning. I also like having fun learning new things and helping the people around me grow together to create a scalable and motivated team.\n",
    "\n",
    "         \n",
    " Working Experience\n",
    "2022-2024 Technical Team Lead in Data Science - PropertyGuru\n",
    "Responsibilities\n",
    "● Led a regional team of 5 data scientists (Singapore, Malaysia, Vietnam).\n",
    "○ Coach the next generation of data scientists and leaders.\n",
    "○ Motivate the team by win-win and rewarding OKRs, which provide clear career growth.\n",
    "● Create an environment to drive innovation\n",
    "○ Create a safe zone to try new things.\n",
    "■ Focus on a growth mindset and learning instead of penalty from failures\n",
    "■ Create a team project portfolio balancing risk and innovation\n",
    "○ Create innovation rituals: knowledge-sharing sessions, tech radar sessions\n",
    "● Plan data science experiment process, system architecture, data management, and engineering excellence (automation, QA, risk management, MLOps).\n",
    "● Coordinate with non-technical stakeholders to identify business opportunities, manage expectations, and structure the action items and timeline.\n",
    "Project: ProxyPrice Automated Valuation System (AVM)\n",
    "● Finance Models: Hedonic Regression, Discounted Cash Flow, and XGBoost\n",
    "● Trust: with feature importance, confidence, and human communication\n",
    "● Traceability:\n",
    "○ Traceable ID columns and logs in every intermediate table in the pipeline.\n",
    "○ Data Lineage documentation\n",
    "● Quality:\n",
    "○ Data cleaning and enrichment: rule-based, statistics-based, and NLP-based\n",
    "○ Automated QA at every step of the pipeline.\n",
    "● Efficiency: Accelerated Batch Prediction: parallel processing with Airflow + Celery + Kubernetes\n",
    "Impact\n",
    "● Prediction coverage improved from 20% to 70%.\n",
    "● Defects reduced by 90%.\n",
    "● Reduced complaint tickets from human misunderstanding by 80%\n",
    "● The customer round trip time was reduced from 40 to 10 days.\n",
    "Project: DataSense AI Assistant Chatbot\n",
    "● AI chatbot that allows enterprise customers to interact with proprietary data.\n",
    "● RAG (Retrieval Augmented Generation) with ChatGPT API and company proprietary data\n",
    "● Automated evaluation metrics: Context Relevance, Groundness, Answer Relevance\n",
    "● Scalability: handle 2000 requests/second\n",
    "Impact: Increase the user engagement by 200% Project: Property Description Tagging\n",
    "● Binary classification models to determine tags from property description using pre-trained LLMs\n",
    "● Prompt engineering techniques: Few-shot examples, Chain-of-thought, etc.\n",
    "● Cost-controlled using the most cost-effective LLM models and prompts\n",
    "● Personal Identifiable Information (PII) is masked using the GCP Data Loss Prevention service.\n",
    "Impact: Recall +60% over the lexicon matching method. Precision +10%\n",
    "     \n",
    "2021-2022 Senior Data Scientist - PropertyGuru\n",
    "Project: Semantic Search\n",
    "● Search by free text, image, checkboxes\n",
    "● Deep learning text embedding using Sentence-BERT\n",
    "    \n",
    " ● Fast vector search with HNSW algorithm using OpenSearch\n",
    "● Near real-time scalable item data syncing with RabbitMQ to OpenSearch\n",
    "● Handle 2000 requests/second with latency < 100 ms\n",
    "Impact: +163% click-through rate on unseen text input Project: AI Media Moderation Engine (AIME)\n",
    "● A computer vision model to detect inappropriate images (e.g., nudity, the face of a person)\n",
    "● Use transfer learning or pre-train models depending on the inappropriate content type.\n",
    "● Ensure scalability with Lambda + AWS SageMaker + AWS SQS\n",
    "Impact: Human moderator workload is reduced by 90%\n",
    "     \n",
    "     \n",
    "2020-2021 Data Scientist - PropertyGuru\n",
    "Project: Recommendation-As-A-Service\n",
    "● Custom deep learning embedding model from item content + user click flow data.\n",
    "● Fast vector search with HNSW algorithm using OpenSearch\n",
    "● Handle 2000 requests/second.\n",
    "Impact: 60% increase in click-through rate (from A/B testing result)\n",
    "\n",
    "     \n",
    "2017-2020: Senior Software Engineer - PropertyGuru\n",
    "Scrum Methodology, Microservice Architecture, CI/CD, Automated Testing\n",
    "Responsibilities\n",
    "● Introduced AI techniques to improve the efficiency of SEO operations.\n",
    "● Designed system architectures for new services that are efficient, scalable, and robust\n",
    "● Set up monitoring metrics to manage risks of system health and project delivery.\n",
    "Project: Human URL Project\n",
    "● URL system using Named Entity Recognition (NER) to free text and voice search\n",
    "● Impact: Reduced website bounce rates from unseen keywords by 50%. Project: Internal Link Recommendation Engine\n",
    "● Developed an AI-powered system for generating internal links\n",
    "● Used Collaborative Filtering recommendation engine\n",
    "● Impact: Team velocity increased from 100 pages per month to 100,000 pages per month\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avin Sharma\n",
      "Senior Associate Consultant - Infosys Limited\n",
      ".\n",
      "Hyderabad, Telangana - Email me on Indeed: indeed.com/r/Avin-Sharma/3ad8a8b57a172613\n",
      ".\n",
      "WORK EXPERIENCE\n",
      ".\n",
      "Senior Associate Consultant\n",
      ".\n",
      "Infosys Limited -\n",
      ".\n",
      "July 2015 to Present\n",
      ".\n",
      "Worked on Presales activities preparing Proposals, RFP's, preparing presentations, budgets/\n",
      "quotations based\n",
      "on client requirements.\n",
      " Responsible for entire sales cycle from market research for prospective clients to final\n",
      "negotiation & sales\n",
      "closure.\n",
      " Worked on mapping commercials to increase the profits and resources planning/ utilization.\n",
      " Leading and coordinating a team of business analysts, developers and testers for execution\n",
      "of multiple projects.\n",
      " Developed strategies and generated business for the firm by building corporate relationships\n",
      "with client.\n",
      ".\n",
      "Senior System Engineer\n",
      ".\n",
      "Infosys Limited -\n",
      ".\n",
      "August 2008 to April 2013\n",
      ".\n",
      "Worked as a quality analyst for client; performed system testing for their customer web portal\n",
      "having modules\n",
      "like Dashboard, Billing, Shop, Payments, Login etc.\n",
      " Led a team of three system engineers; identified competency gap and conducted knowledge\n",
      "sharing sessions\n",
      "for new team members.\n",
      " Communicated with the client team members on frequent basis for understanding their issues\n",
      "and bringing\n",
      "action items to closure.\n",
      " Independently handled the responsibility for designing manual test cases and executing them.\n",
      " Worked on client's merger Integration application as a Developer which involved migration from\n",
      "lower framework\n",
      "to higher framework.\n",
      " Responsible for migrating the part of application developed in dot net from lower version to\n",
      "higher version.\n",
      " Developed the internal portals on Dot Net platform for Infosys team members.\n",
      " Completed the assigned tasks as per the timelines and ensured that the given tasks are\n",
      "completed with compliance to the benchmarks.\n",
      ".\n",
      "https://www.indeed.com/r/Avin-Sharma/3ad8a8b57a172613?isid=rex-download&ikw;=download-top&c\n",
      "o;=IN\n",
      "..\n",
      " Received STAR Certification for showcasing outstanding Soft Skills on 'Business & interpersonal\n",
      "communication\n",
      "for Client delight.\n",
      ".\n",
      "Guru Nanak Dev University\n",
      ".\n",
      "Amritsar, Punjab -\n",
      ".\n",
      "July 2004 to June 2008\n",
      ".\n",
      "EDUCATION\n",
      ".\n",
      "Great Lakes Institute of Management - Chennai, Tamil Nadu\n",
      ".\n",
      "April 2014 to April 2015\n",
      ".\n",
      "SKILLS\n",
      ".\n",
      "Requirement Analysis (Less than 1 year), Sales support (Less than 1 year), Test Planning (Less\n",
      "than 1 year)\n",
      ".\n",
      "ADDITIONAL INFORMATION\n",
      ".\n",
      "Skills\n",
      "Bid management, Sales support, Requirement Analysis, Test Planning and Test execution\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Load the PDF document\n",
    "filepath = 'resume.pdf'\n",
    "filepath = 'sample_resume.pdf'\n",
    "loader = PyPDFLoader(filepath)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Extract text from each page\n",
    "text = \"\"\n",
    "for page in pages:\n",
    "    text += page.page_content\n",
    "\n",
    "decoded_text = text.encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "\n",
    "decoded_text = unidecode(text)\n",
    "\n",
    "resume_content = decoded_text\n",
    "\n",
    "# Print the extracted text\n",
    "# print(text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def do_llm_completion(messages, verbose=False):\n",
    "\n",
    "    url = \"http://localhost:1234/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        # \"model\": \"LM Studio Community/Phi-3-mini-4k-instruct-GGUF\",\n",
    "        \"model\": \"LM Studio Community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": -1,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data), stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        content = \"\"\n",
    "        for chunk in response.iter_content(chunk_size=None):\n",
    "            if chunk:\n",
    "                chunk_line = chunk.decode('utf-8')\n",
    "                if chunk_line.startswith('data:'):\n",
    "                    chunk_content = chunk_line[len('data: '):]\n",
    "                    if not chunk_content.startswith('[DONE]'):\n",
    "                        # print(chunk_content)\n",
    "                        chunk_data = json.loads(chunk_content)\n",
    "                        if chunk_data['choices'][0]['finish_reason'] is None and chunk_data.get('id'):\n",
    "                            # print(chunk_data)\n",
    "                            # print('====', chunk_data['choices'][0]['delta']['role'], '====')\n",
    "                            # print(chunk_data['choices'][0]['delta']['content'])\n",
    "                            content += chunk_data['choices'][0]['delta']['content']\n",
    "        return content\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# messages = [\n",
    "#     # {\"role\": \"system\", \"content\": \"you are resume parser. answer in json\"},\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"\"\"\n",
    "# you are a resume parser. extract jobs positions along with attributes of the position: title, company, start_year, end_year\n",
    "# job positions only, not projects.\n",
    "\n",
    "# example output\n",
    "# [\n",
    "#     {\"position_title\": \"aaa\", \"company\":\"bbb\", \"start_year\": null, \"end_year\": 2023},\n",
    "#     {\"position_title\": \"aaa\", \"company\":\"bbb\", \"start_year\": 2021, \"end_year\": null}\n",
    "# ]\n",
    "#         \"\"\"\n",
    "#     },\n",
    "#     # {\"role\": \"user\", \"content\": resume_content[:3000]},\n",
    "#     {\"role\": \"user\", \"content\": resume_content},\n",
    "#     # {\"role\": \"user\", \"content\": resumes[90]['content']},\n",
    "#     ]\n",
    "# llm_response = do_llm_completion(messages)\n",
    "# print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chunks 1\n",
      "chunk 0: 226 tokens\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "chunker = TokenTextSplitter(chunk_size=1800, chunk_overlap=500)\n",
    "chunks = chunker.split_text(resume['content'])\n",
    "\n",
    "tokenizer = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n",
    "print('number of chunks', len(chunks))\n",
    "for ci in range(len(chunks)):\n",
    "    print(f'chunk {ci}: {len(tokenizer.split_text(chunks[ci]))} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asish Ratha\n",
      "Subject matter Expert - Accenture\n",
      "\n",
      "Chennai, Tamil Nadu - Email me on Indeed: indeed.com/r/Asish-Ratha/853988e0e0e236a3\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Subject matter Expert\n",
      "\n",
      "Accenture -\n",
      "\n",
      "March 2012 to Present\n",
      "\n",
      "Subject matter expert\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "Berhampur university, Khallikote autonomous college -  Brahmapur, Orissa\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Invoice (5 years), posting. (5 years), TRAINING (4 years)\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "SKILLS\n",
      "Invoice processing, Team handling, new joiners training.sap posting,vendor call attend and\n",
      "resolve the issue,meet SLA tat,working with client tool.\n",
      "\n",
      "https://www.indeed.com/r/Asish-Ratha/853988e0e0e236a3?isid=rex-download&ikw=download-top&co=IN\n"
     ]
    }
   ],
   "source": [
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw output\n",
      "Here is the extracted email address in JSON format:\n",
      "\n",
      "[\n",
      "    {\"value\": \"asish.ratha@gmail.com\"}\n",
      "]\n",
      "\n",
      "Note: The email address is inferred from the Indeed link provided, which appears to be a public profile page. However, please note that this may not always be accurate or up-to-date.\n",
      "=== clean output ===\n",
      "[\n",
      "    {\n",
      "        \"value\": \"asish.ratha@gmail.com\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from src.llm.lmstudio import do_llm_completion\n",
    "\n",
    "def llm_extract_email(chunk):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "    you are resume parser.\n",
    "    extract email address of this person from his resume.\n",
    "    output in json\n",
    "\n",
    "    example output\n",
    "    [\n",
    "        { \"value\": \"xxx@gmail.com\" }\n",
    "    ]\n",
    "\n",
    "    if there is no e-mail in his resume. output a blank array like this\n",
    "    [ ]\n",
    "    \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": chunk,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    llm_response = do_llm_completion(messages)\n",
    "\n",
    "    print('raw output')\n",
    "    print(llm_response)\n",
    "\n",
    "    \n",
    "    from src.utils.extract_json import extract_json_only\n",
    "    response_json = extract_json_only(llm_response)\n",
    "\n",
    "\n",
    "    result = []\n",
    "    if len(response_json) > 0:\n",
    "        \n",
    "        result = json.loads(response_json[0])\n",
    "\n",
    "    return result\n",
    "\n",
    "extracted_emails = llm_extract_email(chunk)\n",
    "print('=== clean output ===')\n",
    "print(json.dumps(extracted_emails, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw output\n",
      "Based on the provided information, here is the output in JSON format:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"first_name\": \"Asish\",\n",
      "        \"family_name\": \"Ratha\",\n",
      "        \"nationality\": \"\",\n",
      "        \"birthplace\": \"Orissa\",\n",
      "        \"country_code\": \"IN\",\n",
      "        \"marital_status\": \"\", // assuming not mentioned\n",
      "        \"date_of_birth\": \"\", // assuming not mentioned\n",
      "        \"gender\": \"\" // assuming not specified, but based on the name, it's likely male\n",
      "    }\n",
      "]\n",
      "\n",
      "Note: The nationality is not explicitly mentioned, so I left it blank. Similarly, marital status and date of birth are also not mentioned, so they are left as empty strings.\n",
      "=== clean output ===\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from src.llm.lmstudio import do_llm_completion\n",
    "\n",
    "def llm_extract_person_info(chunk):\n",
    "\n",
    "    messages = [\n",
    "\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"this is a resume of person\\n\\n\" + chunk,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "what's the name of this person? what country he is from? is he married? should he be male or female?\n",
    "output in json.\n",
    "\n",
    "    example output\n",
    "    [\n",
    "        {\n",
    "            \"gender\": \"\",\n",
    "            \"birthplace\": \"Thailand\",\n",
    "            \"first_name\": \"Hello\",\n",
    "            \"family_name\": \"World\",\n",
    "            \"middle_name\": \"\",\n",
    "            \"nationality\": 'Thai',\n",
    "            \"date_of_birth\": \"\",\n",
    "            \"marital_status\": \"\",\n",
    "            \"country_code\": \"TH\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "     \n",
    "    \"\"\"\n",
    "        },\n",
    "\n",
    "    ]\n",
    "\n",
    "    llm_response = do_llm_completion(messages)\n",
    "\n",
    "    print('raw output')\n",
    "    print(llm_response)\n",
    "\n",
    "    \n",
    "    from src.utils.extract_json import extract_json_only\n",
    "    response_json = extract_json_only(llm_response)\n",
    "\n",
    "\n",
    "    result = []\n",
    "    if len(response_json) > 0:\n",
    "        \n",
    "        result = json.loads(response_json[0])\n",
    "\n",
    "    return result\n",
    "\n",
    "extracted_person_infos = llm_extract_person_info(chunk)\n",
    "print('=== clean output ===')\n",
    "print(json.dumps(extracted_person_infos, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw output\n",
      "Here is the extracted email address in JSON format:\n",
      "\n",
      "[\n",
      "    {\"value\": \"asish.ratha@gmail.com\"}\n",
      "]\n",
      "\n",
      "Note: The email address is inferred from the Indeed link provided, which appears to be a public profile page. However, please note that this may not always be accurate or up-to-date.\n",
      "=== clean output ===\n",
      "[\n",
      "    {\n",
      "        \"value\": \"asish.ratha@gmail.com\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from src.llm.lmstudio import do_llm_completion\n",
    "\n",
    "def llm_extract_emails(chunk):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "    you are resume parser.\n",
    "    extract email address of this person from his resume.\n",
    "    output in json\n",
    "\n",
    "    example output\n",
    "    [\n",
    "        { \"value\": \"xxx@gmail.com\" }\n",
    "    ]\n",
    "\n",
    "    if there is no e-mail in his resume. output a blank array like this\n",
    "    [ ]\n",
    "    \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": chunk,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    llm_response = do_llm_completion(messages)\n",
    "\n",
    "    print('raw output')\n",
    "    print(llm_response)\n",
    "\n",
    "    \n",
    "    from src.utils.extract_json import extract_json_only\n",
    "    response_json = extract_json_only(llm_response)\n",
    "\n",
    "\n",
    "    result = []\n",
    "    if len(response_json) > 0:\n",
    "        \n",
    "        result = json.loads(response_json[0])\n",
    "\n",
    "    return result\n",
    "\n",
    "extracted_emails = llm_extract_emails(chunk)\n",
    "print('=== clean output ===')\n",
    "print(json.dumps(extracted_emails, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw output\n",
      "After parsing the resume, I found one phone number:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"type\": \"Telephone\",\n",
      "    \"value\": \"\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note that there is no explicit phone number mentioned in the resume. The email address provided is indeed.com/... which does not contain a phone number.\n",
      "=== clean output ===\n",
      "[\n",
      "    {\n",
      "        \"type\": \"Telephone\",\n",
      "        \"value\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from src.llm.lmstudio import do_llm_completion\n",
    "\n",
    "def llm_extract_phones(chunk):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "you are resume parser.\n",
    "extract phone number of this person from his resume.\n",
    "output in json\n",
    "\n",
    "example output\n",
    "[{ \"type\": \"Telephone\", \"value\": \"4204208484\" }]\n",
    "\n",
    "if there is no phone number in his resume. output a blank array like this\n",
    "[ ]\n",
    "    \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": chunk,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    llm_response = do_llm_completion(messages)\n",
    "\n",
    "    print('raw output')\n",
    "    print(llm_response)\n",
    "\n",
    "    \n",
    "    from src.utils.extract_json import extract_json_only\n",
    "    response_json = extract_json_only(llm_response)\n",
    "\n",
    "\n",
    "    result = []\n",
    "    if len(response_json) > 0:\n",
    "        \n",
    "        result = json.loads(response_json[0])\n",
    "\n",
    "    return result\n",
    "\n",
    "extracted_phones = llm_extract_phones(chunk)\n",
    "print('=== clean output ===')\n",
    "print(json.dumps(extracted_phones, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw output\n",
      "Here is the extracted data in JSON format:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"job_title\": \"Subject matter Expert\",\n",
      "        \"employer\": \"Accenture\",\n",
      "        \"start_date\": \"March 2012\",\n",
      "        \"end_date\": \"Present\",\n",
      "        \"city\": \"\",\n",
      "        \"country_code\": \"\"\n",
      "    }\n",
      "]\n",
      "\n",
      "Note that there is only one job position mentioned in the resume, which is the Subject matter Expert role at Accenture. The city and country code are not specified, so I left them blank. If there were no job positions mentioned in the resume, the output would be an empty array like this:\n",
      "\n",
      "[]\n",
      "=== clean output ===\n",
      "[\n",
      "    {\n",
      "        \"job_title\": \"Subject matter Expert\",\n",
      "        \"employer\": \"Accenture\",\n",
      "        \"start_date\": \"March 2012\",\n",
      "        \"end_date\": \"Present\",\n",
      "        \"city\": \"\",\n",
      "        \"country_code\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from src.llm.lmstudio import do_llm_completion\n",
    "\n",
    "def llm_extract_jobs(chunk):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a resume parser. respond in json\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{chunk}\\n\\n\" + ('*' * 20) + \"\"\"\n",
    "extract his job positions and these attributes related to the position: title, employer, end_date, start_date, city, country_code.\n",
    "no more attributes other than this.\n",
    "output in json.\n",
    "\n",
    "example output\n",
    "[\n",
    "    {\n",
    "      \"city\": \"\",\n",
    "      \"job_title\": \"NCAT ASEC\",\n",
    "      \"country\": \"\",\n",
    "      \"company\": \"Hickman Property Holdings LLC\",\n",
    "      \"end_date\": \"2023-\",\n",
    "      \"start_date\": \"2022-2\",\n",
    "      \"country_code\": \"IN\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "if there is no job position in his resume. output a blank array like this\n",
    "[ ]\n",
    "    \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    llm_response = do_llm_completion(messages)\n",
    "\n",
    "    print('raw output')\n",
    "    print(llm_response)\n",
    "\n",
    "    \n",
    "    from src.utils.extract_json import extract_json_only\n",
    "    response_json = extract_json_only(llm_response)\n",
    "\n",
    "\n",
    "    result = []\n",
    "    if len(response_json) > 0:\n",
    "        \n",
    "        result = json.loads(response_json[0])\n",
    "\n",
    "    return result\n",
    "\n",
    "extracted_jobs = llm_extract_jobs(chunk)\n",
    "print('=== clean output ===')\n",
    "print(json.dumps(extracted_jobs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'employer': 'Accenture', 'start_date': 'March 2012', 'end_date': 'Present', 'city': '', 'country_code': '', 'title': 'Subject matter Expert'}]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def normalize_job_item(item):\n",
    "    result = copy.copy(item)\n",
    "    if 'job_title' in result:\n",
    "        result['title'] = result['job_title']\n",
    "        result.pop('job_title')\n",
    "    if 'company' in result:\n",
    "        result['employer'] = result['company']\n",
    "        result.pop('company')\n",
    "\n",
    "    return result\n",
    "\n",
    "extracted_jobs = [normalize_job_item(item) for item in extracted_jobs]\n",
    "print(extracted_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw output\n",
      "Here are the skills extracted from Asish Ratha's resume:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\"name\": \"Invoice processing\", \"type\": \"hard skill\"},\n",
      "  {\"name\": \"Posting\", \"type\": \"hard skill\"},\n",
      "  {\"name\": \"TRAINING\", \"type\": \"hard skill\"},\n",
      "  {\"name\": \"SAP posting\", \"type\": \"hard skill\"},\n",
      "  {\"name\": \"Vendor call attend and resolve the issue\", \"type\": \"soft skill\"},\n",
      "  {\"name\": \"Meet SLA target\", \"type\": \"soft skill\"},\n",
      "  {\"name\": \"Working with client tools\", \"type\": \"soft skill\"},\n",
      "  {\"name\": \"Team handling\", \"type\": \"soft skill\"},\n",
      "  {\"name\": \"New joiners training\", \"type\": \"soft skill\"}\n",
      "]\n",
      "```\n",
      "\n",
      "Note that I've categorized the skills into hard skills (technical skills) and soft skills (non-technical skills). Let me know if you'd like me to make any changes!\n",
      "=== clean output ===\n",
      "[\n",
      "    {\n",
      "        \"name\": \"Invoice processing\",\n",
      "        \"type\": \"hard skill\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Posting\",\n",
      "        \"type\": \"hard skill\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"TRAINING\",\n",
      "        \"type\": \"hard skill\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"SAP posting\",\n",
      "        \"type\": \"hard skill\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Vendor call attend and resolve the issue\",\n",
      "        \"type\": \"soft skill\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Meet SLA target\",\n",
      "        \"type\": \"soft skill\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Working with client tools\",\n",
      "        \"type\": \"soft skill\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Team handling\",\n",
      "        \"type\": \"soft skill\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"New joiners training\",\n",
      "        \"type\": \"soft skill\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from src.llm.lmstudio import do_llm_completion\n",
    "\n",
    "def llm_extract_skills(chunk):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "you are resume parser.\n",
    "extract his skills as specified in his resume. both hard and soft skills.\n",
    "output in json\n",
    "\n",
    "example output\n",
    "[\n",
    "    {\"name\": \"Community Service\", \"type\": \"soft skill\"},\n",
    "    {\"name\": \"Concord\", \"type\": \"soft skill\"}\n",
    "]\n",
    "\n",
    "if there is no skills in his resume, output a blank array like this\n",
    "[ ]\n",
    "    \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": chunk,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    llm_response = do_llm_completion(messages)\n",
    "\n",
    "    print('raw output')\n",
    "    print(llm_response)\n",
    "\n",
    "    \n",
    "    from src.utils.extract_json import extract_json_only\n",
    "    response_json = extract_json_only(llm_response)\n",
    "\n",
    "\n",
    "    result = []\n",
    "    if len(response_json) > 0:\n",
    "        \n",
    "        result = json.loads(response_json[0])\n",
    "\n",
    "    return result\n",
    "\n",
    "extracted_skills = llm_extract_skills(chunk)\n",
    "print('=== clean output ===')\n",
    "print(json.dumps(extracted_skills, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw output\n",
      "Here is the extracted education information in JSON format:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"city\": \"Brahmapur\",\n",
      "    \"school\": \"Berhampur University, Khallikote Autonomous College\",\n",
      "    \"country\": \"Orissa\",\n",
      "    \"end_date\": \"\",\n",
      "    \"start_date\": \"\",\n",
      "    \"degree_name\": \"\",\n",
      "    \"description\": \"\",\n",
      "    \"country_code\": \"IN\",\n",
      "    \"degree_major\": \"\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note that the education section only mentions one institution, Berhampur University, Khallikote Autonomous College, without specifying a degree name or dates.\n",
      "=== clean output ===\n",
      "[\n",
      "    {\n",
      "        \"city\": \"Brahmapur\",\n",
      "        \"school\": \"Berhampur University, Khallikote Autonomous College\",\n",
      "        \"country\": \"Orissa\",\n",
      "        \"end_date\": \"\",\n",
      "        \"start_date\": \"\",\n",
      "        \"degree_name\": \"\",\n",
      "        \"description\": \"\",\n",
      "        \"country_code\": \"IN\",\n",
      "        \"degree_major\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from src.llm.lmstudio import do_llm_completion\n",
    "\n",
    "def llm_extract_educations(chunk):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "you are resume parser.\n",
    "extract his education as specified in his resume.\n",
    "output in json\n",
    "\n",
    "example output\n",
    "[\n",
    "  {\n",
    "    \"city\": \"\",\n",
    "    \"school\": \"State University\",\n",
    "    \"country\": \"\",\n",
    "    \"end_date\": \"2024-5\",\n",
    "    \"start_date\": \"-\",\n",
    "    \"degree_name\": \"\",\n",
    "    \"description\": \"\",\n",
    "    \"country_code\": \"NC\",\n",
    "    \"degree_major\": \"\"\n",
    "  },\n",
    "  {\n",
    "    \"city\": \"\",\n",
    "    \"school\": \"Central Piedmont Community College\",\n",
    "    \"country\": \"\",\n",
    "    \"end_date\": \"2021-12\",\n",
    "    \"start_date\": \"2020-8\",\n",
    "    \"degree_name\": \"\",\n",
    "    \"description\": \"North Carolina Agricultural and Technical State University, Greensboro, NC\\nMay 2024\\nGPA: 3.4\\nCentral Piedmont Community College, Charlotte NC\\nAugust 2020 - December 2021\\nGPA: 3.3\\nCourse Careers, Remote December 2022 - January 2023\\nGPA: 3.3\\nCourse Careers, Remote December 2022 - January 2023\",\n",
    "    \"country_code\": \"NC\",\n",
    "    \"degree_major\": \"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "if there is no education in his resume, output a blank array like this\n",
    "[ ]\n",
    "    \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": chunk,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    llm_response = do_llm_completion(messages)\n",
    "\n",
    "    print('raw output')\n",
    "    print(llm_response)\n",
    "\n",
    "    \n",
    "    from src.utils.extract_json import extract_json_only\n",
    "    response_json = extract_json_only(llm_response)\n",
    "\n",
    "\n",
    "    result = []\n",
    "    if len(response_json) > 0:\n",
    "        \n",
    "        result = json.loads(response_json[0])\n",
    "\n",
    "    return result\n",
    "\n",
    "extracted_educations = llm_extract_educations(chunk)\n",
    "print('=== clean output ===')\n",
    "print(json.dumps(extracted_educations, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {\n",
    "\t\"personal\": {\n",
    "    \"gender\": \"\",\n",
    "    \"full_name\": \"Hidden\",\n",
    "    \"birthplace\": \"\",\n",
    "    \"first_name\": \"Hidden\",\n",
    "    \"family_name\": \"Hidden\",\n",
    "    \"middle_name\": \"Hidden\",\n",
    "    \"nationality\": [],\n",
    "    \"picture_url\": None,\n",
    "    \"date_of_birth\": \"\",\n",
    "    \"marital_status\": \"\",\n",
    "    \"picture_extension\": None\n",
    "  },\n",
    "  \"contact\": {\n",
    "    \"email\": [],\n",
    "    \"phone\": [],\n",
    "    \"address\": [],\n",
    "    \"website\": []\n",
    "  },\n",
    "  \"summary\": {\n",
    "    \"benefits\": \"\",\n",
    "    \"objective\": \"\",\n",
    "    \"description\": \"\",\n",
    "    \"notice_period\": \"\",\n",
    "    \"current_salary\": \"\"\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"job_pk\": 463152,\n",
    "    \"remark\": \"Parsing Complete\",\n",
    "    \"status\": \"succeeded\",\n",
    "    \"resume_pk\": 15801024,\n",
    "    \"candidate_pk\": 21380189,\n",
    "    \"language_code\": \"en\",\n",
    "    \"language_confidence\": 0.9999951854043442,\n",
    "  },\n",
    "  \"education\": [],\n",
    "  \"experience\": [],\n",
    "\t\"languages\": [],\n",
    "  \"skills\": [],\n",
    "  \"achievements\": {},\n",
    "  \"certifications\": {},\n",
    "  \"qualifications\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data['skills'] = [item['name'] for item in extracted_skills]\n",
    "output_data['education'] = extracted_educations\n",
    "output_data['experience'] = extracted_jobs\n",
    "output_data['contact']['email'] = extracted_emails\n",
    "output_data['contact']['phone'] = extracted_phones\n",
    "output_data['personal'] = extracted_person_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"personal\": [],\n",
      "    \"contact\": {\n",
      "        \"email\": [\n",
      "            {\n",
      "                \"value\": \"asish.ratha@gmail.com\"\n",
      "            }\n",
      "        ],\n",
      "        \"phone\": [\n",
      "            {\n",
      "                \"type\": \"Telephone\",\n",
      "                \"value\": \"\"\n",
      "            }\n",
      "        ],\n",
      "        \"address\": [],\n",
      "        \"website\": []\n",
      "    },\n",
      "    \"summary\": {\n",
      "        \"benefits\": \"\",\n",
      "        \"objective\": \"\",\n",
      "        \"description\": \"\",\n",
      "        \"notice_period\": \"\",\n",
      "        \"current_salary\": \"\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "        \"job_pk\": 463152,\n",
      "        \"remark\": \"Parsing Complete\",\n",
      "        \"status\": \"succeeded\",\n",
      "        \"resume_pk\": 15801024,\n",
      "        \"candidate_pk\": 21380189,\n",
      "        \"language_code\": \"en\",\n",
      "        \"language_confidence\": 0.9999951854043442\n",
      "    },\n",
      "    \"education\": [\n",
      "        {\n",
      "            \"city\": \"Brahmapur\",\n",
      "            \"school\": \"Berhampur University, Khallikote Autonomous College\",\n",
      "            \"country\": \"Orissa\",\n",
      "            \"end_date\": \"\",\n",
      "            \"start_date\": \"\",\n",
      "            \"degree_name\": \"\",\n",
      "            \"description\": \"\",\n",
      "            \"country_code\": \"IN\",\n",
      "            \"degree_major\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"experience\": [\n",
      "        {\n",
      "            \"employer\": \"Accenture\",\n",
      "            \"start_date\": \"March 2012\",\n",
      "            \"end_date\": \"Present\",\n",
      "            \"city\": \"\",\n",
      "            \"country_code\": \"\",\n",
      "            \"title\": \"Subject matter Expert\"\n",
      "        }\n",
      "    ],\n",
      "    \"languages\": [],\n",
      "    \"skills\": [\n",
      "        \"Invoice processing\",\n",
      "        \"Posting\",\n",
      "        \"TRAINING\",\n",
      "        \"SAP posting\",\n",
      "        \"Vendor call attend and resolve the issue\",\n",
      "        \"Meet SLA target\",\n",
      "        \"Working with client tools\",\n",
      "        \"Team handling\",\n",
      "        \"New joiners training\"\n",
      "    ],\n",
      "    \"achievements\": {},\n",
      "    \"certifications\": {},\n",
      "    \"qualifications\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(output_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:29.126817\n"
     ]
    }
   ],
   "source": [
    "elapsed_seconds = datetime.datetime.now() - start_time\n",
    "print(elapsed_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected endpoint or method. (POST /completions)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[268], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuggest me a good name for an ice cream parlour that is located on a beach!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Generate text using the LM Studio model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1086\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1081\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1084\u001b[0m     )\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1086\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m   1096\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain_core/language_models/llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m     ]\n\u001b[0;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain_core/language_models/llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain_core/language_models/llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 657\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain_openai/llms/base.py:361\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# Sometimes the AI Model calling will get error, we should raise it.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Otherwise, the next code 'choices.extend(response[\"choices\"])'\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# will throw a \"TypeError: 'NoneType' object is not iterable\" error\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# to mask the true error. Because 'response[\"choices\"]' is None.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    363\u001b[0m choices\u001b[38;5;241m.\u001b[39mextend(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    364\u001b[0m _update_token_usage(_keys, response, token_usage)\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected endpoint or method. (POST /completions)"
     ]
    }
   ],
   "source": [
    "# from langchain.llms import LlamaCpp\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Initialize the LlamaCpp model with the LM Studio API URL\n",
    "llm = OpenAI(openai_api_base=\"127.0.0.1:1234\", model_name=\"LM Studio Community/Meta-Llama-3-8B-Instruct-GGUF\", openai_api_key='noneed')\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Suggest me a good name for an ice cream parlour that is located on a beach!\"\n",
    "\n",
    "# Generate text using the LM Studio model\n",
    "print(llm(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
